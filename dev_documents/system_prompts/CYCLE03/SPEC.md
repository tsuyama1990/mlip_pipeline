# Cycle 03 Specification: Trainer Phase (Pacemaker Integration)

## 1. Summary

Cycle 03 integrates the core machine learning engine: **Pacemaker**. This module is responsible for training the Atomic Cluster Expansion (ACE) potential using the ground-truth data generated by the Oracle in Cycle 02.

The key functionalities required are:
1.  **Data Management**: Converting raw ASE Atoms objects (with Energy/Forces) into the specific `.pckl.gzip` format required by Pacemaker.
2.  **Training Orchestration**: Wrapping the `pace_train` command line tool to execute fitting jobs, managing hyperparameters like cutoff radii and B-basis functions via the `TrainingConfig`.
3.  **Active Set Selection**: Implementing the D-Optimality criterion using `pace_activeset`. This is crucial for the "Active Learning" aspect—instead of training on all accumulating data (which becomes slow), we select a representative subset that maximizes information.

This cycle establishes the "Learning" part of the loop.

## 2. System Architecture

We populate the `trainer/` directory.

### 2.1 File Structure

```ascii
src/mlip_autopipec/
├── config/
│   └── schemas/
│       └── **training.py**         # Pacemaker-specific settings
├── **trainer/**
│   ├── **__init__.py**
│   ├── **pacemaker.py**            # Wrapper for pace commands
│   ├── **dataset.py**              # Data conversion logic
│   └── **baseline.py**             # Logic for LJ/ZBL baseline setup
└── orchestration/
    └── phases/
        └── **training.py**         # Training Phase Logic
```

## 3. Design Architecture

### 3.1 Dataset Manager (`trainer/dataset.py`)

*   **Responsibility**: Maintain the persistence of training data.
*   **Methods**:
    *   `save_to_pacemaker_format(atoms_list: List[Atoms], path: Path)`: Writes the pickle file.
    *   `merge_datasets(main_path: Path, new_data_path: Path) -> Path`: Appends new DFT results to the main dataset.

### 3.2 Pacemaker Wrapper (`trainer/pacemaker.py`)

*   **Responsibility**: Interface with the `pace_*` CLI tools.
*   **Key Methods**:
    *   `train(dataset_path: Path, config: TrainingConfig) -> Path`: Constructs the `input.yaml` for Pacemaker and calls `pace_train`. Returns the path to the resulting `potential.yace`.
    *   `select_active_set(dataset_path: Path, selection_size: int) -> Path`: Calls `pace_activeset` to filter the dataset.

### 3.3 Baseline Configuration (`trainer/baseline.py`)

*   **Responsibility**: Generate the reference potential files.
*   Pacemaker allows defining a reference potential (e.g., `pair_style lj/cut`). This class generates the necessary LAMMPS-style parameter block to be included in the training config, ensuring the ACE model learns the *difference* ($E_{DFT} - E_{Ref}$).

## 4. Implementation Approach

1.  **Step 1: Dataset Conversion.**
    *   Investigate Pacemaker's expected input format. It usually accepts a pandas DataFrame pickled or ASE atoms. We will implement `ase_to_pckl` conversion.

2.  **Step 2: Training Config Generation.**
    *   Create a template for `input.yaml` (Pacemaker's config).
    *   Map `mlip_autopipec.config.TrainingConfig` values (cutoff, order, etc.) into this YAML template.

3.  **Step 3: `pace_train` Execution.**
    *   Use `subprocess.run` to call `pace_train input.yaml`.
    *   Capture stdout/stderr to log training progress (RMSE per epoch).

4.  **Step 4: Active Set Logic.**
    *   Implement the call to `pace_activeset`. This command typically takes a large dataset and outputs a smaller one. We need to handle the file paths correctly.

## 5. Test Strategy

### 5.1 Unit Testing
*   **Config Mapping:**
    *   Test that `TrainingConfig` correctly generates a valid YAML string for Pacemaker.
    *   Verify that `ZBL` parameters are correctly formatted if selected.
*   **Dataset Conversion:**
    *   Create a few ASE Atoms with random forces.
    *   Call `save_to_pacemaker_format`.
    *   (If possible) Load the pickle back and verify data integrity.

### 5.2 Integration Testing
*   **Mock Training:**
    *   Since `pace_train` might be slow or require a GPU, we will mock the subprocess call.
    *   The mock should verify that the correct arguments were passed and create a dummy `potential.yace` file so the pipeline can continue.
*   **CLI Integration:**
    *   Test the `ActiveLearningManager` (from Cycle 01/02) calling the Trainer to "update" the potential.
