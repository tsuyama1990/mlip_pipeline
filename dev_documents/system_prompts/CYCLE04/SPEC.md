# Cycle 04 Specification: Trainer & Pacemaker Integration

## 1. Summary
Cycle 04 integrates the core machine learning engine, "Pacemaker" (Atomic Cluster Expansion), into the system. The `Trainer` component will be responsible for taking the labeled `Dataset` and producing a `Potential` file (`.yace`). Key features include "Active Set Optimization" (selecting the most informative structures) and "Delta Learning" (learning the difference between DFT and a physical baseline like Lennard-Jones to ensure robustness).

## 2. System Architecture

### 2.1 File Structure
**Bold** files are to be created or modified in this cycle.

```ascii
.
├── src/
│   └── mlip_autopipec/
│       ├── components/
│       │   ├── trainer/
│       │   │   ├── **pacemaker_trainer.py**  # Wrapper for pace_train
│       │   │   ├── **active_set.py**         # Wrapper for pace_activeset
│       │   │   └── **delta_learning.py**     # Baseline potential logic
│       ├── domain_models/
│       │   └── **trainer_config.py**         # Config for training (epochs, radius, etc.)
│       └── utils/
│           └── **pacemaker_utils.py**        # Helper to parse pacemaker logs/output
```

## 3. Design Architecture

### 3.1 Trainer Component (`src/mlip_autopipec/components/trainer/`)

*   **`PacemakerTrainer` Class**:
    *   Inherits from `BaseTrainer`.
    *   **Data Preparation**: Converts `Dataset` (JSONL) to Pacemaker's expected input format (`.pckl.gzip` or `data.pckl`).
    *   **Execution**: Constructs the `pace_train` command line arguments based on `TrainerConfig` and executes it via `subprocess`.
    *   **Delta Learning**:
        *   If `baseline_type="lj"`, it first computes the LJ energy/forces for all structures in the dataset.
        *   It subtracts these baseline values from the DFT labels.
        *   It configures Pacemaker to learn the residual.
    *   **Output**: Returns a `Potential` object pointing to the generated `.yace` file.

*   **Active Set Optimization (`active_set.py`)**:
    *   Before training, runs `pace_activeset` to select a subset of structures that maximize the information matrix determinant (D-optimality).
    *   This reduces training time and improves potential transferability.

### 3.2 Configuration (`TrainerConfig`)
*   `cutoff`: Float (e.g., 5.0 Å).
*   `max_deg`: Int (polynomial degree).
*   `batch_size`: Int.
*   `max_num_epochs`: Int.
*   `baseline_type`: "None", "lj", "zbl".

## 4. Implementation Approach

1.  **Baseline Logic**: Implement `compute_lj_baseline(structure)` using `ase.calculators.lj`.
2.  **Data Conversion**: Implement `dataset_to_pacemaker(dataset, path)`.
3.  **Pacemaker Wrapper**:
    *   Implement `PacemakerTrainer.train()`.
    *   Use `subprocess.run(["pace_train", ...], check=True)`.
    *   Capture stdout/stderr for logging.
4.  **Active Set**:
    *   Implement `select_active_set(dataset)`.

## 5. Test Strategy

### 5.1 Unit Testing
*   **Baseline**: Verify that `compute_lj_baseline` returns correct positive energy for overlapping atoms (repulsion).
*   **Command Generation**: Verify that `PacemakerTrainer` generates the correct command string (e.g., including `--fitting_weight_energy`).

### 5.2 Integration Testing
*   **Mock Pacemaker**: Since `pace_train` might not be installed in the CI environment, we need a `MockTrainer` (already exists from Cycle 1) but improved to actually produce a dummy `.yace` file (just a text file).
*   **Real Pacemaker (Local)**: If `pace_train` is available:
    1.  Create a small dataset of 10 perturbed Silicon structures.
    2.  Run `PacemakerTrainer.train()`.
    3.  Assert `potential.yace` is created.
    4.  Assert the fitting error (RMSE) reported in `report.yaml` (generated by pacemaker) is below a threshold.
