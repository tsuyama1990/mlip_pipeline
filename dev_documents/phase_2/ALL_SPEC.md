機械学習ポテンシャル自動生成・解析パイプライン (MLIP-AutoPipe) システム仕様書System Architecture & Requirement Definition Document|| 項目 | 内容 || Project Name | MLIP-AutoPipe (The "Zero-Human" Protocol) || Version | 4.1.0 (Detailed Specification) || Status | Implementation Phase || Author | System Architect (AI Assistant) || Target Audience | Backend Engineers, Materials Scientists, HPC Administrators |目次イントロダクションと設計哲学1.1 背景：スケーラビリティのパラドックスと「鶏と卵」問題の解消1.2 システムの目的：完全自律型（Zero-Human）ワークフロー1.3 コア技術戦略：非同期OTF、周期埋め込み、サロゲート先行探索システム全体アーキテクチャ2.1 ハイレベルデータフロー図2.2 技術スタックとライブラリ選定機能要件詳細：自律駆動エンジン3.1 Module A: Physics-Informed Generator (物理則に基づく初期構造生成)A.1 合金：SQS + 歪み + RattlingA.2 分子：Normal Mode Sampling (NMS)A.3 共有結合・イオン結晶：Melt-Quench & Defect Engineering3.2 Module B: Surrogate Explorer (基盤モデルによる高速探索)B.1 MACE-MPによるDirect SamplingB.2 Farthest Point Sampling (FPS) による選抜3.3 Module C: Automated DFT Factory (高耐久Quantum Espresso実行環境)C.1 Static Calculation Protocol (静的計算の鉄則)C.2 ヒューリスティック・パラメータ決定 (SSSP, 磁性, K点)C.3 自動エラーリカバリ・ロジック (Auto-Recovery)3.4 Module D: Active Learning & Training (Pacemaker連携)D.1 Delta Learningとハイパーパラメータ最適化D.2 不確実性定量化 (MaxVol / Extrapolation Grade)3.5 Module E: Scalable Inference & OTF (MD/kMC/Embedding)E.1 Decoupled Inference (推論と学習の分離)E.2 Periodic Embedding Strategy (周期埋め込み)E.3 Force Masking (バッファ領域のマスク処理)データ構造とインターフェース4.1 ユーザー入力定義 (Minimal Config)4.2 データベーススキーマとメタデータ非機能要件と運用5.1 エラーハンドリングとリソース管理5.2 スケーラビリティと並列実行実装ロードマップ1. イントロダクションと設計哲学1.1 背景：スケーラビリティのパラドックスと「鶏と卵」問題物質科学シミュレーションにおいて、第一原理計算（DFT）の精度と分子動力学（MD）のスケールはトレードオフの関係にある。機械学習ポテンシャル（MLIP）はこの壁を打破するが、構築には「高品質な教師データ」が必要であり、その生成には「高品質なサンプリング（AIMD等）」が必要となる。しかし、AIMDを実行するには「ある程度のポテンシャル」が必要という**「鶏と卵（Chicken and Egg）」の問題**が存在する。 さらに、従来のAIMDによるデータ生成は、時間相関が高く情報密度の低いデータを大量に生成するため、計算コスト（$O(N^3)$）に対して学習効率が著しく悪い。1.2 システムの目的：Zero-Human Intervention本システム「MLIP-AutoPipe」は、初期構造生成から能動学習、長時間シミュレーションに至る全工程から**「人間を完全に排除（Zero-Human）」**することを目的とする。専門知識を持たないユーザーでも「物質」と「目的」を入力するだけで、システムが自律的に学習と推論のループを回し、物理的知見（相図、拡散係数など）を出力する。1.3 コア技術戦略本システムは、添付資料にある最新の技術トレンドに基づき、以下の3つの戦略を採用する。Surrogate-First Strategy (サロゲート先行探索): 初期探索に高コストなDFTやAIMDを使わず、汎用基盤モデル（MACE-MP, M3GNet等）を「スカウト」として利用する（Direct Sampling）。これにより、DFT計算なしで物理的に妥当な候補構造を数百万件生成し、その中から情報価の高い構造のみを選抜する。Decoupled Inference & Training (非同期OTF): シミュレーション（推論）とDFT計算（学習）を完全に分離する。推論エンジンが不確実な構造を検知してキューに積み、学習エンジンが非同期に処理してモデルを更新する。これによりシミュレーションを中断させない。Periodic Embedding with Force Masking (周期埋め込みと力マスク): 局所的な学習データを切り出す際、従来の「クラスター切り出し（真空境界）」ではなく、**「周辺環境を含んだ小さな周期境界ボックス」**として切り出す。さらに、境界付近の原子（バッファ層）の力を学習データから除外（マスク）することで、表面効果アーティファクトを排除し、バルク環境の高精度な学習を実現する。2. システム全体アーキテクチャ2.1 ハイレベルデータフロー図graph TD
    User[User: Minimal Config] --> Heuristic[Heuristic Engine]
    Heuristic --> FullConfig[System: Full Execution Config]

    subgraph "Phase 1: Cold Start (The Seed)"
        FullConfig --> Generator[Module A: Physics-Informed Generator]
        Generator -- SQS/NMS/Melt --> Surrogate[Module B: MACE Surrogate]
        Surrogate --> FPS[Selector: Farthest Point Sampling]
    end

    subgraph "Phase 2: Training Loop (The Factory)"
        FPS --> DFT_Queue
        Embed_Queue --> DFT_Queue
        DFT_Queue --> QE[Module C: Automated DFT Factory]
        QE -- Error --> Recovery[Auto-Recovery Logic]
        Recovery --> QE
        QE --> DB[(ASE Database: .extxyz)]
        DB --> Pacemaker[Module D: Pacemaker Trainer]
        Pacemaker --> Potential[Active Potential (.yace)]
    end

    subgraph "Phase 3: Production (The Explorer)"
        Potential --> Inference[Module E: MD/kMC Engine]
        Inference -- Uncertainty > Threshold --> Extractor[Periodic Embedding]
        Extractor --> Embed_Queue[Priority Queue]
        Inference -- Uncertainty OK --> Analysis[Physical Properties]
    end

2.2 技術スタック選定Core Logic: Python 3.10+Environment: uv (Rust製高速パッケージマネージャ)MLIP Framework: Pacemaker (ACE形式。解釈性と計算速度のバランスにより選定)DFT Engine: Quantum Espresso (オープンソース、自動化ライブラリの充実度)MD Engine: LAMMPS (Pacemaker公式サポート、大規模並列化)Surrogate Model: MACE (PyTorchベースの汎用モデル、Direct Sampling用)Structure Tools: ASE (Atomic Simulation Environment), Pymatgen, icet (SQS生成)3. 機能要件詳細：自律駆動エンジン3.1 Module A: Physics-Informed Generator (物理則に基づく初期構造生成)目的: DFT計算ゼロの状態から、AIMDに頼らずに物理的に多様な初期構造セットを作成する。Pacemakerは「補間器」であるため、物理的な極限状態（原子の接近、結合の解離）を明示的に教え込む必要がある。A.1 合金 (Alloys): SQS + 歪み + RattlingSQS (Special Quasirandom Structures):icet または mcsqs を使用。指定組成（例: Fe70Ni30）に対し、最もランダムに近い相関関数を持つスーパーセル（32〜96原子）を生成する。プロトコル: ターゲット組成だけでなく、周辺組成（例: Fe60Ni40, Fe80Ni20）のSQSも生成し、局所的な濃度揺らぎを学習させる。Lattice Strain (弾性学習):生成したSQSに対し、等方的体積変化（$\pm 1\% \sim \pm 10\%$）と、せん断変形（Monoclinic/Triclinic box deformation）を加える。これにより状態方程式（EOS）と弾性定数（$C_{11}, C_{12}, C_{44}$）を学習させる。Rattling (熱振動模倣):原子位置にランダムなガウスノイズを加える。Small Rattling ($\sigma = 0.01 - 0.05 \text{\AA}$): 調和振動領域（Phonon）の学習。Large Rattling ($\sigma = 0.1 - 0.3 \text{\AA}$): 非調和領域および高温安定性の学習。A.2 分子 (Molecules): Normal Mode Sampling (NMS)背景: 分子動力学（MD）では、高周波数の結合伸縮（Stiff modes）や稀なねじれ構造を効率的にサンプリングできない。NMSプロトコル:低精度DFTまたは半経験的法で構造最適化し、Hessian（力の定数行列）を計算。Hessianの固有ベクトル（基準振動モード）に沿って構造を変形させる。各モードに対し、300K, 600K, 1000K相当のエネルギーを持つ変位を与えた構造を生成する。これにより、MDを行わずに振動空間を均一に網羅する。Dimer Scans: 分子間の反発壁（Pauli repulsion）を学習させるため、2つの分子を強制的に接近させた配置（距離 $0.5\text{\AA} \sim 6.0\text{\AA}$）を追加する。A.3 共有結合・イオン結晶: Melt-Quench & Defect EngineeringMelt-Quench (サロゲート利用):共有結合結晶（Si, Cなど）の結合組み換えを学習するため、MACEを用いて高温溶融（3000K）$\to$ 急冷（Quench）のシミュレーションを行い、液体およびアモルファス構造を抽出する。これらは多様な配位数（Coordination numbers）を含む。Defect Engineering ("One Defect" Strategy):完全結晶だけでなく、Pymatgenを用いて以下の欠陥構造を自動生成する。Vacancy: 1原子除去。Interstitial: 四面体/八面体空隙への原子挿入。Antisite: 異種原子の入れ替え（合金・化合物の場合）。これらを緩和（Short Relaxation）させた構造を含めることで、欠陥形成エネルギーの精度を担保する。3.2 Module B: Surrogate Explorer (基盤モデルによる高速探索)目的: DFTリソースを節約するため、初期探索を汎用学習済みモデルで行う「Direct Sampling」アプローチ。B.1 MACE-MPによるDirect SamplingModule Aで生成した数千〜数万の候補構造に対し、DFT計算を行わずに MACE-MP-0 (Materials Projectで学習済み) を用いてエネルギー・力・ストレスを予測、あるいは短時間のMDを実行する。物理的にあり得ない崩壊（原子間距離が異常に近いなど）を起こした構造をこの段階で排除する。B.2 Farthest Point Sampling (FPS) による選抜MACEで処理された構造群から、ACE記述子またはSOAP記述子を用いて構造フィンガープリントを計算する。FPSアルゴリズム: 既存のデータセットから「記述子空間上で最も遠い」構造を順次選択する。これにより、数百万の候補から、最も多様性に富んだ（情報価の高い）数百〜数千の構造のみをDFT計算キューに送る。3.3 Module C: Automated DFT Factory (高耐久Quantum Espresso実行環境)目的: 人間の介入なしに、MLIP学習に最適な「Static Force Calculation（一点計算）」を完遂する。C.1 Static Calculation Protocol (静的計算の鉄則)Calculation Type: calculation = 'scf' を厳守。relax（構造緩和）は絶対に行わない。入力された「歪んだ構造」そのものが学習データであるため。Essential Flags:tprnfor = .true.: Hellmann-Feynman力の出力。tstress = .true.: ストレス（Virial）の出力。弾性特性の学習に必須。disk_io = 'low': 巨大な波動関数ファイルの書き出しを抑制し、I/O負荷を低減。C.2 ヒューリスティック・パラメータ決定SSSP Protocol (Standard Solid State Pseudopotentials):元素ごとに検証済みの擬ポテンシャルとカットオフ値のライブラリ（SSSP）を使用。Cutoff: 系に含まれる全元素の中で最大の推奨カットオフ値 ($E_{cut}^{wfc}$) を採用する。Mode: 通常は Efficiency モードで十分だが、フォノン計算等の高精度が必要な場合は Precision モードを選択可能にする。Smearing (金属・絶縁体の自動処理):系が金属か絶縁体か不明な場合、常に occupations = 'smearing' を使用する。Type: smearing = 'mv' (Marzari-Vanderbilt)。Width: degauss = 0.01 ~ 0.02 Ry。これにより金属の収束を助けつつ、絶縁体の物理も大きく損なわない。Magnetism (磁性の自動判定):Suspect List: 遷移金属（V, Cr, Mn, Fe, Co, Ni等）や希土類が含まれる場合、自動的に nspin = 2 （スピン分極）を設定。Initialization: 初期磁気モーメント (starting_magnetization) を強磁性（Ferromagnetic）設定で0.2~0.5程度与え、非磁性解への落下を防ぐ。K-points:Linear Density Heuristic を採用。逆格子空間での密度 $\rho_k \approx 0.15 \text{\AA}^{-1}$ を基準に、セルサイズに応じてK点メッシュを自動生成する。C.3 自動エラーリカバリ・ロジック (Auto-Recovery)計算が収束しない場合、以下の優先順位でパラメータを緩和して再投入するロジックを実装する。| Priority | Error Type | Action | Parameter Change || 1 | Convergence Fail | Mixing緩和 | mixing_beta: $0.7 \to 0.3 \to 0.1$ || 2 | Convergence Fail | Preconditioner変更 | mixing_mode: plain $\to$ local-tf || 3 | Convergence Fail | 電子温度上昇 | degauss: $+0.02 \text{Ry}$ (一時的) || 4 | Cholesky Error | アルゴリズム変更 | diagonalization: david $\to$ cg (Conjugate Gradient) |3.4 Module D: Active Learning & Training (Pacemaker連携)目的: データの蓄積に合わせてポテンシャルを更新する。D.1 Delta Learningとハイパーパラメータ最適化Delta Learning: エネルギーを $E_{total} = E_{ZBL} + E_{ACE}$ として分解し、近距離核反発（ZBLポテンシャル）をベースラインとして学習する。これにより、高エネルギー衝突時の物理的整合性を保つ。Loss Weights: エネルギー、力、ストレスの重み付けを動的に調整するが、基本推奨値は Energy: 1.0, Force: 100.0, Stress: 10.0 とする。力が最も重要な情報源である。D.2 不確実性定量化 (MaxVol / Extrapolation Grade)Pacemakerの extrapolation_grade ($\gamma$) を使用。これはD-optimality基準に基づき、現在の構造が学習データの基底関数空間からどれだけ「はみ出しているか」を測定する。Threshold:$\gamma < 2.0$: 既知（信頼できる）。$2.0 \le \gamma \le 10.0$: 候補（DFT計算へ送る）。$\gamma > 10.0$: 危険（物理的に崩壊している可能性が高いため、棄却するかtimestepを小さくして再試行）。3.5 Module E: Scalable Inference & OTF (MD/kMC/Embedding)目的: 本番シミュレーション中に未知領域を探索し、精度を向上させる。E.1 Decoupled Inference (推論と学習の分離)シミュレーション（MD/kMC）は学習完了を待たずに走り続ける。不確実な構造が見つかった場合、そのスナップショットを「Embedding Queue」に投げ、バックグラウンドでDFT計算と学習が行われる。新しいポテンシャルができ次第、推論エンジンにホットスワップ（または次回のジョブから適用）される。E.2 Periodic Embedding Strategy (周期埋め込み)局所的な反応イベントを切り出す際、「クラスター（真空）」ではなく「周期境界ボックス」を使用する。Selection: 不確実性が高い原子 $i$ を中心とする。Box Definition: カットオフ半径 $r_c$ にバッファ層 $\delta_{buffer}$ を加えたサイズ $L \approx 2(r_c + \delta_{buffer})$ の立方体セルを定義する。推奨値: $r_c \approx 5.0\text{\AA}, \delta_{buffer} \approx 3.0 \sim 4.0\text{\AA}$。Extraction: 元の巨大な系から、このボックス内に含まれる原子を抽出し、新たな周期境界条件を持つ小規模セル（数十〜百原子程度）として再構築する。E.3 Force Masking (力マスク処理)問題: 切り出された周期セルの境界付近にある原子（バッファ領域）は、元の系とは異なる環境（人工的な周期境界の影響）を受けているため、そのDFT力は「正解」ではない。解決策: 学習データ登録時、バッファ領域にある原子の**Weightを0（マスク）**にする。中心領域（半径 $r_c$ 以内）の原子の力のみを教師データとして学習させる。これにより、人工的な境界の影響を排除しつつ、バルク環境の電子状態を正しく反映した力を学習できる。4. データ構造とインターフェース4.1 ユーザー入力定義 (Minimal Config: input.yaml)ユーザーは専門用語を避け、物質と目的のみを記述する。project_name: "FeNi_Alloy_Diffusion"

target_system:
  elements: ["Fe", "Ni"]
  composition: { "Fe": 0.7, "Ni": 0.3 }
  crystal_structure: "fcc" # またはCIFファイルのパス

simulation_goal:
  type: "melt_quench" # 'elastic', 'phase_diagram', 'diffusion' など
  temperature_range: [300, 1500] # K
  
resources:
  dft_code: "quantum_espresso"
  parallel_cores: 64
  gpu_enabled: true

4.2 データベーススキーマ (ASE-db拡張)SQLite/PostgreSQLを使用し、以下のメタデータを管理する。| Column | Type | Description || id | int | Primary Key || uuid | string | Unique Structure ID || energy | float | DFT Total Energy (eV) || forces | blob | Force array (N x 3) || stress | blob | Virial Stress Tensor (3 x 3) || config_type | text | sqs_strain, nms_300k, active_learning_gen3 等の出自 || uncertainty_gamma | float | 追加時のExtrapolation Grade || force_mask | blob | Mask array (N). 1.0 for core, 0.0 for buffer. |5. 非機能要件と運用5.1 エラーハンドリングとリソース管理Zombie Job Killer: 外部DFTプロセスが応答しなくなった場合、タイムアウト（例: 4時間）で強制終了し、パラメータを緩和してリトライキューに入れる。Disk Cleanup: DFT計算で生成される巨大な波動関数ファイル（.wfc, .hub）は、計算成功直後に即時削除する設定をデフォルトとする（disk_io='low' と併用）。5.2 スケーラビリティTask Queue: Celery または Dask を用いて、数千のDFTジョブと学習ジョブを非同期に管理する。Checkpointing: すべての状態（現在のポテンシャル、済みデータ、未処理キュー）はDBに永続化され、システムダウン時も途中から再開可能にする。6. 実装ロードマップPhase 1: The Foundation (基盤構築)DFT Factory: ASEをラップし、SSSP/磁性/エラー回復を自動処理する QERunner クラスの実装。Generator: icet によるSQS生成と、Pymatgen による欠陥導入の実装。Phase 2: The Surrogate & Trainer (高速化と学習)MACE Integration: 初期構造をMACEで事前計算し、FPSで選抜するパイプラインの実装。Pacemaker Automation: 学習設定ファイルの自動生成と、学習ジョブの実行管理。Phase 3: The Intelligence (OTF & Embedding)OTF Loop: LAMMPS実行 $\to$ 不確実性検知 $\to$ 停止/抽出のループ実装。Embedding Logic: 周期ボックス切り出しとForce Masking生成アルゴリズムの実装。Phase 4: UI & ReleaseCLI: mlip-auto run input.yaml コマンドの整備。Dashboard: 学習曲線（RMSE）、現在の構造、不確実性ヒストグラムを可視化する簡易Web UI。