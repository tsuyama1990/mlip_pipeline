============================= test session starts ==============================
platform linux -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0
rootdir: /app
configfile: pyproject.toml
testpaths: tests
plugins: mock-3.15.1, cov-7.0.0
collected 73 items

tests/integration/test_dft_integration.py .....                          [  6%]
tests/integration/test_monitoring_integration.py ...                     [ 10%]
tests/modules/test_config_generator.py .                                 [ 12%]
tests/modules/test_dft.py ..FF..                                         [ 20%]
tests/modules/test_dft_heuristics.py .......                             [ 30%]
tests/modules/test_exploration.py .....ss                                [ 39%]
tests/modules/test_explorer.py ....                                      [ 45%]
tests/modules/test_generator.py ....                                     [ 50%]
tests/modules/test_inference.py ...                                      [ 54%]
tests/modules/test_training.py ....                                      [ 60%]
tests/test_app.py ...                                                    [ 64%]
tests/test_config.py .....                                               [ 71%]
tests/test_config_factory.py .                                           [ 72%]
tests/test_database.py ...                                               [ 76%]
tests/test_logging.py .                                                  [ 78%]
tests/test_monitoring.py ...                                             [ 82%]
tests/test_workflow_manager.py .......                                   [ 91%]
tests/utils/test_resilience.py ..F...
WARNING: Failed to generate report: No data to report.

                                                                         [100%]

=================================== FAILURES ===================================
_________________ test_dft_runner_raises_dft_calculation_error _________________

self = <mlip_autopipec.modules.dft.DFTRunner object at 0x7f6ccb1f0fe0>
job = DFTJob(atoms=Atoms(symbols='Si2', pbc=True, cell=[[0.0, 2.715, 2.715], [2.715, 0.0, 2.715], [2.715, 2.715, 0.0]]), params=<MagicMock spec='DFTInputParameters' id='140105241011952'>, job_id=UUID('fb1efbab-db3f-4dab-ab9b-b4f36650f686'))

    @retry(
        attempts=3,
        delay=5.0,
        exceptions=(DFTCalculationError, subprocess.CalledProcessError),
        on_retry=dft_retry_handler,
    )
    def run(self, job: DFTJob) -> DFTResult:
        """
        Runs a DFTJob, handling transient errors with the @retry decorator.

        Args:
            job: The DFT job configuration.

        Returns:
            DFTResult: The parsed results of the calculation.

        Raises:
            DFTCalculationError: If the calculation fails after retries.
        """
        if job is None or not isinstance(job, DFTJob):
            msg = "A valid DFTJob object is required to run a calculation."
            raise ValueError(msg)

        # Checking existence of attributes is defensive coding against
        # potential dynamic modification or incomplete mocking in tests.
        if not getattr(job, "params", None):
            msg = "DFTJob must have initialized parameters."
            raise ValueError(msg)

        if getattr(job, "atoms", None) is None:
            msg = "DFTJob must have a valid 'atoms' attribute."
            raise ValueError(msg)

        if getattr(job, "job_id", None) is None:
             # Should not happen with default_factory but good for robustness
             msg = "DFTJob must have a valid 'job_id'."
             raise ValueError(msg)

        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                work_dir = Path(temp_dir)
                input_path = work_dir / "espresso.pwi"
                output_path = work_dir / "espresso.pwo"

                self.input_generator.prepare_input_files(work_dir, job.atoms, job.params)
>               self.process_runner.execute(input_path, output_path)

mlip_autopipec/modules/dft.py:193:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1139: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1143: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1202: in _execute_mock_call
    raise result
mlip_autopipec/modules/dft.py:193: in run
    self.process_runner.execute(input_path, output_path)
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1139: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1143: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1202: in _execute_mock_call
    raise result
mlip_autopipec/modules/dft.py:193: in run
    self.process_runner.execute(input_path, output_path)
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1139: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1143: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <MagicMock name='mock.execute' id='140105241010992'>
args = (PosixPath('/tmp/tmpplpznxx7/espresso.pwi'), PosixPath('/tmp/tmpplpznxx7/espresso.pwo'))
kwargs = {}, effect = <list_iterator object at 0x7f6ccb1f26b0>
result = CalledProcessError(1, 'pw.x')

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method

        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
>                   raise result
E                   subprocess.CalledProcessError: Command 'pw.x' returned non-zero exit status 1.

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1202: CalledProcessError

The above exception was the direct cause of the following exception:

args = (<mlip_autopipec.modules.dft.DFTRunner object at 0x7f6ccb1f0fe0>, DFTJob(atoms=Atoms(symbols='Si2', pbc=True, cell=[[0...arams=<MagicMock spec='DFTInputParameters' id='140105241011952'>, job_id=UUID('fb1efbab-db3f-4dab-ab9b-b4f36650f686')))
kwargs = {}, current_kwargs = {}, attempt = 3
msg = 'Max retries (3) exceeded for run.', new_kwargs = None

    @wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        current_kwargs = kwargs.copy()
        for attempt in range(1, attempts + 1):
            try:
>               return func(*args, **current_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

mlip_autopipec/utils/resilience.py:64:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mlip_autopipec.modules.dft.DFTRunner object at 0x7f6ccb1f0fe0>
job = DFTJob(atoms=Atoms(symbols='Si2', pbc=True, cell=[[0.0, 2.715, 2.715], [2.715, 0.0, 2.715], [2.715, 2.715, 0.0]]), params=<MagicMock spec='DFTInputParameters' id='140105241011952'>, job_id=UUID('fb1efbab-db3f-4dab-ab9b-b4f36650f686'))

    @retry(
        attempts=3,
        delay=5.0,
        exceptions=(DFTCalculationError, subprocess.CalledProcessError),
        on_retry=dft_retry_handler,
    )
    def run(self, job: DFTJob) -> DFTResult:
        """
        Runs a DFTJob, handling transient errors with the @retry decorator.

        Args:
            job: The DFT job configuration.

        Returns:
            DFTResult: The parsed results of the calculation.

        Raises:
            DFTCalculationError: If the calculation fails after retries.
        """
        if job is None or not isinstance(job, DFTJob):
            msg = "A valid DFTJob object is required to run a calculation."
            raise ValueError(msg)

        # Checking existence of attributes is defensive coding against
        # potential dynamic modification or incomplete mocking in tests.
        if not getattr(job, "params", None):
            msg = "DFTJob must have initialized parameters."
            raise ValueError(msg)

        if getattr(job, "atoms", None) is None:
            msg = "DFTJob must have a valid 'atoms' attribute."
            raise ValueError(msg)

        if getattr(job, "job_id", None) is None:
             # Should not happen with default_factory but good for robustness
             msg = "DFTJob must have a valid 'job_id'."
             raise ValueError(msg)

        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                work_dir = Path(temp_dir)
                input_path = work_dir / "espresso.pwi"
                output_path = work_dir / "espresso.pwo"

                self.input_generator.prepare_input_files(work_dir, job.atoms, job.params)
                self.process_runner.execute(input_path, output_path)
                result = self.output_parser.parse(output_path, job.job_id)

                logger.info(f"DFT job {job.job_id} succeeded.")
                return result

        except DFTCalculationError:
            # Let domain-specific errors propagate directly
            raise

        except subprocess.CalledProcessError as e:
            # Wrap low-level subprocess errors with context
            msg = f"DFT subprocess failed for job {job.job_id}"
>           raise DFTCalculationError(
                msg,
                stdout=getattr(e, "stdout", ""),
                stderr=getattr(e, "stderr", ""),
            ) from e
E           mlip_autopipec.exceptions.DFTCalculationError: DFT subprocess failed for job fb1efbab-db3f-4dab-ab9b-b4f36650f686
E           --- STDOUT ---
E           some other error
E           --- STDERR ---

mlip_autopipec/modules/dft.py:206: DFTCalculationError

The above exception was the direct cause of the following exception:

dft_runner = <mlip_autopipec.modules.dft.DFTRunner object at 0x7f6ccb1f0fe0>
mock_process_runner = <MagicMock spec='QEProcessRunner' id='140105241009840'>

    @patch("mlip_autopipec.modules.dft.retry", lambda attempts, delay, exceptions, on_retry: lambda f: f)
    def test_dft_runner_raises_dft_calculation_error(
        dft_runner,
        mock_process_runner,
    ):
        """Test that DFTRunner raises DFTCalculationError on failure."""
        atoms = bulk("Si")
        error = subprocess.CalledProcessError(1, "pw.x")
        error.stdout = "some other error"
        error.stderr = ""
        # The side_effect needs to be an iterable for each call
        mock_process_runner.execute.side_effect = [error, error, error]
        params = MagicMock(spec=DFTInputParameters)
        job = DFTJob(atoms=atoms, params=params)

        # DFTRunner now wraps CalledProcessError into DFTCalculationError
        with pytest.raises(DFTCalculationError):
>           dft_runner.run(job)

tests/modules/test_dft.py:122:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<mlip_autopipec.modules.dft.DFTRunner object at 0x7f6ccb1f0fe0>, DFTJob(atoms=Atoms(symbols='Si2', pbc=True, cell=[[0...arams=<MagicMock spec='DFTInputParameters' id='140105241011952'>, job_id=UUID('fb1efbab-db3f-4dab-ab9b-b4f36650f686')))
kwargs = {}, current_kwargs = {}, attempt = 3
msg = 'Max retries (3) exceeded for run.', new_kwargs = None

    @wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        current_kwargs = kwargs.copy()
        for attempt in range(1, attempts + 1):
            try:
                return func(*args, **current_kwargs)
            except exceptions as e:
                if attempt == attempts:
                    logger.error(
                        "Function %s failed after %d attempts.",
                        func.__name__,
                        attempts,
                        exc_info=True,
                    )
                    msg = f"Max retries ({attempts}) exceeded for {func.__name__}."
>                   raise MaxRetriesExceededError(msg) from e
E                   mlip_autopipec.exceptions.MaxRetriesExceededError: Max retries (3) exceeded for run.

mlip_autopipec/utils/resilience.py:74: MaxRetriesExceededError
------------------------------ Captured log call -------------------------------
WARNING  mlip_autopipec.utils.resilience:resilience.py:75 Attempt 1/3 for run failed with DFTCalculationError.
WARNING  mlip_autopipec.utils.resilience:resilience.py:75 Attempt 2/3 for run failed with DFTCalculationError.
ERROR    mlip_autopipec.utils.resilience:resilience.py:67 Function run failed after 3 attempts.
Traceback (most recent call last):
  File "/app/mlip_autopipec/modules/dft.py", line 193, in run
    self.process_runner.execute(input_path, output_path)
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1139, in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1143, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1202, in _execute_mock_call
    raise result
  File "/app/mlip_autopipec/modules/dft.py", line 193, in run
    self.process_runner.execute(input_path, output_path)
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1139, in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1143, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1202, in _execute_mock_call
    raise result
  File "/app/mlip_autopipec/modules/dft.py", line 193, in run
    self.process_runner.execute(input_path, output_path)
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1139, in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1143, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1202, in _execute_mock_call
    raise result
subprocess.CalledProcessError: Command 'pw.x' returned non-zero exit status 1.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/mlip_autopipec/utils/resilience.py", line 64, in wrapper
    return func(*args, **current_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/mlip_autopipec/modules/dft.py", line 206, in run
    raise DFTCalculationError(
mlip_autopipec.exceptions.DFTCalculationError: DFT subprocess failed for job fb1efbab-db3f-4dab-ab9b-b4f36650f686
--- STDOUT ---
some other error
--- STDERR ---
______________________ test_process_runner_logs_on_error _______________________

caplog = <_pytest.logging.LogCaptureFixture object at 0x7f6ccb1ed7c0>
tmp_path = PosixPath('/tmp/pytest-of-jules/pytest-27/test_process_runner_logs_on_er0')

    def test_process_runner_logs_on_error(caplog, tmp_path):
        """Verify that the QEProcessRunner logs stdout/stderr on failure."""
        mock_profile = MagicMock(spec=EspressoProfile)
        mock_profile.get_command.return_value = ["non_existent_command"]
        runner = QEProcessRunner(profile=mock_profile)

        input_file = tmp_path / "test.in"
        output_file = tmp_path / "test.out"
        input_file.touch()

        with pytest.raises(FileNotFoundError):
            runner.execute(input_file, output_file)
>       assert "QE executable not found" in caplog.text
E       AssertionError: assert 'QE executable not found' in ''
E        +  where '' = <_pytest.logging.LogCaptureFixture object at 0x7f6ccb1ed7c0>.text

tests/modules/test_dft.py:137: AssertionError
_____________________ test_retry_fails_after_max_attempts ______________________

args = (), kwargs = {}, current_kwargs = {}, attempt = 3
msg = 'Max retries (3) exceeded for mock_func.'

    @wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        current_kwargs = kwargs.copy()
        for attempt in range(1, attempts + 1):
            try:
>               return func(*args, **current_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

mlip_autopipec/utils/resilience.py:64:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1139: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1143: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <Mock id='140105054887216'>, args = (), kwargs = {}
effect = <class 'test_resilience.CustomError'>

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method

        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
>               raise effect
E               test_resilience.CustomError

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1198: CustomError

The above exception was the direct cause of the following exception:

    def test_retry_fails_after_max_attempts():
        """Tests that the decorator gives up and re-raises the exception."""
        mock_func = Mock(side_effect=CustomError)
        mock_func.__name__ = "mock_func"
        decorated_func = retry(attempts=3, delay=0.1, exceptions=(CustomError,))(mock_func)
        with pytest.raises(CustomError):
>           decorated_func()

tests/utils/test_resilience.py:42:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (), kwargs = {}, current_kwargs = {}, attempt = 3
msg = 'Max retries (3) exceeded for mock_func.'

    @wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        current_kwargs = kwargs.copy()
        for attempt in range(1, attempts + 1):
            try:
                return func(*args, **current_kwargs)
            except exceptions as e:
                if attempt == attempts:
                    logger.error(
                        "Function %s failed after %d attempts.",
                        func.__name__,
                        attempts,
                        exc_info=True,
                    )
                    msg = f"Max retries ({attempts}) exceeded for {func.__name__}."
>                   raise MaxRetriesExceededError(msg) from e
E                   mlip_autopipec.exceptions.MaxRetriesExceededError: Max retries (3) exceeded for mock_func.

mlip_autopipec/utils/resilience.py:74: MaxRetriesExceededError
----------------------------- Captured stdout call -----------------------------
           WARNING  Attempt 1/3 for mock_func failed with       resilience.py:75
                    CustomError.
           INFO     Retrying in 0.10 seconds...                 resilience.py:98
           WARNING  Attempt 2/3 for mock_func failed with       resilience.py:75
                    CustomError.
           INFO     Retrying in 0.10 seconds...                 resilience.py:98
           ERROR    Function mock_func failed after 3 attempts. resilience.py:67
                    ╭─── Traceback (most recent call last) ───╮
                    │ /app/mlip_autopipec/utils/resilience.py │
                    │ :64 in wrapper                          │
                    │                                         │
                    │    61 │   │   │   current_kwargs = kwar │
                    │    62 │   │   │   for attempt in range( │
                    │    63 │   │   │   │   try:              │
                    │ ❱  64 │   │   │   │   │   return func(* │
                    │    65 │   │   │   │   except exceptions │
                    │    66 │   │   │   │   │   if attempt == │
                    │    67 │   │   │   │   │   │   logger.er │
                    │                                         │
                    │ /home/jules/.pyenv/versions/3.12.12/lib │
                    │ /python3.12/unittest/mock.py:1139 in    │
                    │ __call__                                │
                    │                                         │
                    │   1136 │   │   # in the signature       │
                    │   1137 │   │   self._mock_check_sig(*ar │
                    │   1138 │   │   self._increment_mock_cal │
                    │ ❱ 1139 │   │   return self._mock_call(* │
                    │   1140 │                                │
                    │   1141 │                                │
                    │   1142 │   def _mock_call(self, /, *arg │
                    │                                         │
                    │ /home/jules/.pyenv/versions/3.12.12/lib │
                    │ /python3.12/unittest/mock.py:1143 in    │
                    │ _mock_call                              │
                    │                                         │
                    │   1140 │                                │
                    │   1141 │                                │
                    │   1142 │   def _mock_call(self, /, *arg │
                    │ ❱ 1143 │   │   return self._execute_moc │
                    │   1144 │                                │
                    │   1145 │   def _increment_mock_call(sel │
                    │   1146 │   │   self.called = True       │
                    │                                         │
                    │ /home/jules/.pyenv/versions/3.12.12/lib │
                    │ /python3.12/unittest/mock.py:1198 in    │
                    │ _execute_mock_call                      │
                    │                                         │
                    │   1195 │   │   effect = self.side_effec │
                    │   1196 │   │   if effect is not None:   │
                    │   1197 │   │   │   if _is_exception(eff │
                    │ ❱ 1198 │   │   │   │   raise effect     │
                    │   1199 │   │   │   elif not _callable(e │
                    │   1200 │   │   │   │   result = next(ef │
                    │   1201 │   │   │   │   if _is_exception │
                    ╰─────────────────────────────────────────╯
                    CustomError
------------------------------ Captured log call -------------------------------
WARNING  mlip_autopipec.utils.resilience:resilience.py:75 Attempt 1/3 for mock_func failed with CustomError.
INFO     mlip_autopipec.utils.resilience:resilience.py:98 Retrying in 0.10 seconds...
WARNING  mlip_autopipec.utils.resilience:resilience.py:75 Attempt 2/3 for mock_func failed with CustomError.
INFO     mlip_autopipec.utils.resilience:resilience.py:98 Retrying in 0.10 seconds...
ERROR    mlip_autopipec.utils.resilience:resilience.py:67 Function mock_func failed after 3 attempts.
Traceback (most recent call last):
  File "/app/mlip_autopipec/utils/resilience.py", line 64, in wrapper
    return func(*args, **current_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1139, in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1143, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py", line 1198, in _execute_mock_call
    raise effect
test_resilience.CustomError
=============================== warnings summary ===============================
.venv/lib/python3.12/site-packages/e3nn/o3/_wigner.py:10
  /app/.venv/lib/python3.12/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.
    _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))

tests/modules/test_exploration.py:128
  /app/tests/modules/test_exploration.py:128: PytestUnknownMarkWarning: Unknown pytest.mark.integration - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.integration

tests/modules/test_exploration.py:138
  /app/tests/modules/test_exploration.py:138: PytestUnknownMarkWarning: Unknown pytest.mark.integration - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.integration

tests/modules/test_inference.py:70
  /app/tests/modules/test_inference.py:70: PytestUnknownMarkWarning: Unknown pytest.mark.integration - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.integration

tests/test_workflow_manager.py::test_save_checkpoint
  /app/.venv/lib/python3.12/site-packages/distributed/node.py:188: UserWarning:

  Port 8787 is already in use.
  Perhaps you already have a cluster running?
  Hosting the HTTP server on port 39701 instead

tests/test_workflow_manager.py::test_load_checkpoint
  /app/.venv/lib/python3.12/site-packages/distributed/node.py:188: UserWarning:

  Port 8787 is already in use.
  Perhaps you already have a cluster running?
  Hosting the HTTP server on port 44871 instead

tests/test_workflow_manager.py::test_init_does_not_load_state_unnecessarily
  /app/.venv/lib/python3.12/site-packages/distributed/node.py:188: UserWarning:

  Port 8787 is already in use.
  Perhaps you already have a cluster running?
  Hosting the HTTP server on port 39197 instead

tests/test_workflow_manager.py::test_resubmit_pending_jobs
  /app/.venv/lib/python3.12/site-packages/distributed/node.py:188: UserWarning:

  Port 8787 is already in use.
  Perhaps you already have a cluster running?
  Hosting the HTTP server on port 33345 instead

tests/test_workflow_manager.py::test_checkpoint_training_history
  /app/.venv/lib/python3.12/site-packages/distributed/node.py:188: UserWarning:

  Port 8787 is already in use.
  Perhaps you already have a cluster running?
  Hosting the HTTP server on port 37389 instead

tests/test_workflow_manager.py::test_checkpoint_training_history
  /app/.venv/lib/python3.12/site-packages/distributed/node.py:188: UserWarning:

  Port 8787 is already in use.
  Perhaps you already have a cluster running?
  Hosting the HTTP server on port 42941 instead

tests/test_workflow_manager.py::test_perform_training
  /app/.venv/lib/python3.12/site-packages/distributed/node.py:188: UserWarning:

  Port 8787 is already in use.
  Perhaps you already have a cluster running?
  Hosting the HTTP server on port 42897 instead

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
_______________ coverage: platform linux, python 3.12.12-final-0 _______________

=========================== short test summary info ============================
FAILED tests/modules/test_dft.py::test_dft_runner_raises_dft_calculation_error
FAILED tests/modules/test_dft.py::test_process_runner_logs_on_error - Asserti...
FAILED tests/utils/test_resilience.py::test_retry_fails_after_max_attempts - ...
======= 3 failed, 68 passed, 2 skipped, 11 warnings in 182.61s (0:03:02) =======
[12:29:18] INFO     Retire worker addresses                    scheduler.py:7614
                    (stimulus_id='retire-workers-1768652958.35
                    03473') (0, 1, 2, 3)
           INFO     Closing Nanny at 'tcp://127.0.0.1:40817'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:34371'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:36957'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:32959'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:39858; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:39852; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:39872; closing.
[12:29:19] INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:39838; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:39075  scheduler.py:5444
                    name: 0
                    (stimulus_id='handle-worker-cleanup-176865
                    2959.066228')
           INFO     Remove worker addr: tcp://127.0.0.1:32781  scheduler.py:5444
                    name: 1
                    (stimulus_id='handle-worker-cleanup-176865
                    2959.1080914')
           INFO     Remove worker addr: tcp://127.0.0.1:39539  scheduler.py:5444
                    name: 2
                    (stimulus_id='handle-worker-cleanup-176865
                    2959.128871')
           INFO     Remove worker addr: tcp://127.0.0.1:36633  scheduler.py:5444
                    name: 3
                    (stimulus_id='handle-worker-cleanup-176865
                    2959.1597264')
           INFO     Lost all workers                           scheduler.py:5582
           INFO     Nanny at 'tcp://127.0.0.1:40817' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:36957' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:34371' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:32959' closed.        nanny.py:626
           INFO     Closing scheduler. Reason: unknown         scheduler.py:4343
           INFO     Scheduler closing all comms                scheduler.py:4371
           INFO     Retire worker addresses                    scheduler.py:7614
                    (stimulus_id='retire-workers-1768652959.69
                    99764') (0, 1, 2, 3)
           INFO     Closing Nanny at 'tcp://127.0.0.1:37971'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:33473'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:42183'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:38579'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:45250; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:45254; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:45268; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:36129  scheduler.py:5444
                    name: 0
                    (stimulus_id='handle-worker-cleanup-176865
                    2959.8932042')
           INFO     Remove worker addr: tcp://127.0.0.1:44975  scheduler.py:5444
                    name: 1
                    (stimulus_id='handle-worker-cleanup-176865
                    2959.9186323')
           INFO     Remove worker addr: tcp://127.0.0.1:37473  scheduler.py:5444
                    name: 2
                    (stimulus_id='handle-worker-cleanup-176865
                    2959.9361334')
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:45256; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:32947  scheduler.py:5444
                    name: 3
                    (stimulus_id='handle-worker-cleanup-176865
                    2959.9997618')
[12:29:20] INFO     Lost all workers                           scheduler.py:5582
           INFO     Batched Comm Closed <TCP (closed) Scheduler   batched.py:122
                    connection to worker
                    local=tcp://127.0.0.1:39383
                    remote=tcp://127.0.0.1:45256>
                    ╭──── Traceback (most recent call last) ────╮
                    │ /app/.venv/lib/python3.12/site-packages/d │
                    │ istributed/batched.py:115 in              │
                    │ _background_send                          │
                    │                                           │
                    │   112 │   │   │   │   │   │   payload, se │
                    │   113 │   │   │   │   │   )               │
                    │   114 │   │   │   │   ) as coro:          │
                    │ ❱ 115 │   │   │   │   │   nbytes = yield  │
                    │   116 │   │   │   │   if nbytes < 1e6:    │
                    │   117 │   │   │   │   │   self.recent_mes │
                    │   118 │   │   │   │   else:               │
                    │                                           │
                    │ /app/.venv/lib/python3.12/site-packages/t │
                    │ ornado/gen.py:783 in run                  │
                    │                                           │
                    │   780 │   │   │   │   self.future = None  │
                    │   781 │   │   │   │   try:                │
                    │   782 │   │   │   │   │   try:            │
                    │ ❱ 783 │   │   │   │   │   │   value = fut │
                    │   784 │   │   │   │   │   except Exceptio │
                    │   785 │   │   │   │   │   │   # Save the  │
                    │   786 │   │   │   │   │   │   # gen.throw │
                    │                                           │
                    │ /app/.venv/lib/python3.12/site-packages/d │
                    │ istributed/comm/tcp.py:263 in write       │
                    │                                           │
                    │   260 │   async def write(self, msg, seri │
                    │   261 │   │   stream = self.stream        │
                    │   262 │   │   if stream is None:          │
                    │ ❱ 263 │   │   │   raise CommClosedError() │
                    │   264 │   │                               │
                    │   265 │   │   frames = await to_frames(   │
                    │   266 │   │   │   msg,                    │
                    ╰───────────────────────────────────────────╯
                    CommClosedError
           INFO     Nanny at 'tcp://127.0.0.1:33473' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:37971' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:42183' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:38579' closed.        nanny.py:626
           INFO     Closing scheduler. Reason: unknown         scheduler.py:4343
           INFO     Scheduler closing all comms                scheduler.py:4371
           INFO     Retire worker addresses                    scheduler.py:7614
                    (stimulus_id='retire-workers-1768652960.60
                    55248') (0, 1, 2, 3)
           INFO     Closing Nanny at 'tcp://127.0.0.1:41579'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:37835'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:44025'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:35429'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:58884; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:58906; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:58890; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:37323  scheduler.py:5444
                    name: 0
                    (stimulus_id='handle-worker-cleanup-176865
                    2960.72485')
           INFO     Remove worker addr: tcp://127.0.0.1:41543  scheduler.py:5444
                    name: 1
                    (stimulus_id='handle-worker-cleanup-176865
                    2960.7413952')
           INFO     Remove worker addr: tcp://127.0.0.1:45727  scheduler.py:5444
                    name: 2
                    (stimulus_id='handle-worker-cleanup-176865
                    2960.75991')
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:58898; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:34701  scheduler.py:5444
                    name: 3
                    (stimulus_id='handle-worker-cleanup-176865
                    2960.798438')
           INFO     Lost all workers                           scheduler.py:5582
           INFO     Nanny at 'tcp://127.0.0.1:41579' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:37835' closed.        nanny.py:626
[12:29:21] INFO     Nanny at 'tcp://127.0.0.1:35429' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:44025' closed.        nanny.py:626
           INFO     Closing scheduler. Reason: unknown         scheduler.py:4343
           INFO     Scheduler closing all comms                scheduler.py:4371
           INFO     Retire worker addresses                    scheduler.py:7614
                    (stimulus_id='retire-workers-1768652961.13
                    60064') (0, 1, 2, 3)
           INFO     Closing Nanny at 'tcp://127.0.0.1:45387'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:43595'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:43511'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:34319'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:40486; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:40482; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:40502; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:40903  scheduler.py:5444
                    name: 1
                    (stimulus_id='handle-worker-cleanup-176865
                    2961.2447202')
           INFO     Remove worker addr: tcp://127.0.0.1:41325  scheduler.py:5444
                    name: 0
                    (stimulus_id='handle-worker-cleanup-176865
                    2961.2538855')
           INFO     Remove worker addr: tcp://127.0.0.1:43815  scheduler.py:5444
                    name: 2
                    (stimulus_id='handle-worker-cleanup-176865
                    2961.2667058')
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:40492; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:41499  scheduler.py:5444
                    name: 3
                    (stimulus_id='handle-worker-cleanup-176865
                    2961.2984092')
           INFO     Lost all workers                           scheduler.py:5582
           INFO     Batched Comm Closed <TCP (closed) Scheduler   batched.py:122
                    connection to worker
                    local=tcp://127.0.0.1:41375
                    remote=tcp://127.0.0.1:40492>
                    ╭──── Traceback (most recent call last) ────╮
                    │ /app/.venv/lib/python3.12/site-packages/d │
                    │ istributed/batched.py:115 in              │
                    │ _background_send                          │
                    │                                           │
                    │   112 │   │   │   │   │   │   payload, se │
                    │   113 │   │   │   │   │   )               │
                    │   114 │   │   │   │   ) as coro:          │
                    │ ❱ 115 │   │   │   │   │   nbytes = yield  │
                    │   116 │   │   │   │   if nbytes < 1e6:    │
                    │   117 │   │   │   │   │   self.recent_mes │
                    │   118 │   │   │   │   else:               │
                    │                                           │
                    │ /app/.venv/lib/python3.12/site-packages/t │
                    │ ornado/gen.py:783 in run                  │
                    │                                           │
                    │   780 │   │   │   │   self.future = None  │
                    │   781 │   │   │   │   try:                │
                    │   782 │   │   │   │   │   try:            │
                    │ ❱ 783 │   │   │   │   │   │   value = fut │
                    │   784 │   │   │   │   │   except Exceptio │
                    │   785 │   │   │   │   │   │   # Save the  │
                    │   786 │   │   │   │   │   │   # gen.throw │
                    │                                           │
                    │ /app/.venv/lib/python3.12/site-packages/d │
                    │ istributed/comm/tcp.py:263 in write       │
                    │                                           │
                    │   260 │   async def write(self, msg, seri │
                    │   261 │   │   stream = self.stream        │
                    │   262 │   │   if stream is None:          │
                    │ ❱ 263 │   │   │   raise CommClosedError() │
                    │   264 │   │                               │
                    │   265 │   │   frames = await to_frames(   │
                    │   266 │   │   │   msg,                    │
                    ╰───────────────────────────────────────────╯
                    CommClosedError
           INFO     Nanny at 'tcp://127.0.0.1:45387' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:34319' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:43511' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:43595' closed.        nanny.py:626
           INFO     Closing scheduler. Reason: unknown         scheduler.py:4343
           INFO     Scheduler closing all comms                scheduler.py:4371
           INFO     Retire worker addresses                    scheduler.py:7614
                    (stimulus_id='retire-workers-1768652961.84
                    35879') (0, 1, 2, 3)
           INFO     Closing Nanny at 'tcp://127.0.0.1:44013'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:42965'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:40433'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:41863'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:54702; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:54728; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:54690; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:35911  scheduler.py:5444
                    name: 0
                    (stimulus_id='handle-worker-cleanup-176865
                    2961.92996')
           INFO     Remove worker addr: tcp://127.0.0.1:40835  scheduler.py:5444
                    name: 2
                    (stimulus_id='handle-worker-cleanup-176865
                    2961.9421544')
           INFO     Remove worker addr: tcp://127.0.0.1:33735  scheduler.py:5444
                    name: 1
                    (stimulus_id='handle-worker-cleanup-176865
                    2961.9540203')
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:54712; closing.
[12:29:22] INFO     Remove worker addr: tcp://127.0.0.1:35709  scheduler.py:5444
                    name: 3
                    (stimulus_id='handle-worker-cleanup-176865
                    2962.005655')
           INFO     Lost all workers                           scheduler.py:5582
           INFO     Nanny at 'tcp://127.0.0.1:44013' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:40433' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:42965' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:41863' closed.        nanny.py:626
           INFO     Closing scheduler. Reason: unknown         scheduler.py:4343
           INFO     Scheduler closing all comms                scheduler.py:4371
           INFO     Retire worker addresses                    scheduler.py:7614
                    (stimulus_id='retire-workers-1768652962.24
                    23043') (0, 1, 2, 3)
           INFO     Closing Nanny at 'tcp://127.0.0.1:36701'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:46343'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:38527'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:46561'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:59522; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:48088; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:48062; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:41451  scheduler.py:5444
                    name: 0
                    (stimulus_id='handle-worker-cleanup-176865
                    2962.3126178')
           INFO     Remove worker addr: tcp://127.0.0.1:38011  scheduler.py:5444
                    name: 1
                    (stimulus_id='handle-worker-cleanup-176865
                    2962.3187988')
           INFO     Remove worker addr: tcp://127.0.0.1:40119  scheduler.py:5444
                    name: 2
                    (stimulus_id='handle-worker-cleanup-176865
                    2962.3295135')
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:48076; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:41173  scheduler.py:5444
                    name: 3
                    (stimulus_id='handle-worker-cleanup-176865
                    2962.3484457')
           INFO     Lost all workers                           scheduler.py:5582
           INFO     Batched Comm Closed <TCP (closed) Scheduler   batched.py:122
                    connection to worker
                    local=tcp://127.0.0.1:35855
                    remote=tcp://127.0.0.1:48076>
                    ╭──── Traceback (most recent call last) ────╮
                    │ /app/.venv/lib/python3.12/site-packages/d │
                    │ istributed/batched.py:115 in              │
                    │ _background_send                          │
                    │                                           │
                    │   112 │   │   │   │   │   │   payload, se │
                    │   113 │   │   │   │   │   )               │
                    │   114 │   │   │   │   ) as coro:          │
                    │ ❱ 115 │   │   │   │   │   nbytes = yield  │
                    │   116 │   │   │   │   if nbytes < 1e6:    │
                    │   117 │   │   │   │   │   self.recent_mes │
                    │   118 │   │   │   │   else:               │
                    │                                           │
                    │ /app/.venv/lib/python3.12/site-packages/t │
                    │ ornado/gen.py:783 in run                  │
                    │                                           │
                    │   780 │   │   │   │   self.future = None  │
                    │   781 │   │   │   │   try:                │
                    │   782 │   │   │   │   │   try:            │
                    │ ❱ 783 │   │   │   │   │   │   value = fut │
                    │   784 │   │   │   │   │   except Exceptio │
                    │   785 │   │   │   │   │   │   # Save the  │
                    │   786 │   │   │   │   │   │   # gen.throw │
                    │                                           │
                    │ /app/.venv/lib/python3.12/site-packages/d │
                    │ istributed/comm/tcp.py:263 in write       │
                    │                                           │
                    │   260 │   async def write(self, msg, seri │
                    │   261 │   │   stream = self.stream        │
                    │   262 │   │   if stream is None:          │
                    │ ❱ 263 │   │   │   raise CommClosedError() │
                    │   264 │   │                               │
                    │   265 │   │   frames = await to_frames(   │
                    │   266 │   │   │   msg,                    │
                    ╰───────────────────────────────────────────╯
                    CommClosedError
           INFO     Nanny at 'tcp://127.0.0.1:36701' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:46343' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:38527' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:46561' closed.        nanny.py:626
           INFO     Closing scheduler. Reason: unknown         scheduler.py:4343
           INFO     Scheduler closing all comms                scheduler.py:4371
           INFO     Retire worker addresses                    scheduler.py:7614
                    (stimulus_id='retire-workers-1768652962.69
                    27166') (0, 1, 2, 3)
           INFO     Closing Nanny at 'tcp://127.0.0.1:33775'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:39307'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:43873'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:34537'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:59166; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:59150; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:59178; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:33899  scheduler.py:5444
                    name: 0
                    (stimulus_id='handle-worker-cleanup-176865
                    2962.7441802')
           INFO     Remove worker addr: tcp://127.0.0.1:34011  scheduler.py:5444
                    name: 1
                    (stimulus_id='handle-worker-cleanup-176865
                    2962.7493567')
           INFO     Remove worker addr: tcp://127.0.0.1:42943  scheduler.py:5444
                    name: 2
                    (stimulus_id='handle-worker-cleanup-176865
                    2962.7537482')
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:59186; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:39431  scheduler.py:5444
                    name: 3
                    (stimulus_id='handle-worker-cleanup-176865
                    2962.7637348')
           INFO     Lost all workers                           scheduler.py:5582
           INFO     Nanny at 'tcp://127.0.0.1:33775' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:43873' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:34537' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:39307' closed.        nanny.py:626
           INFO     Closing scheduler. Reason: unknown         scheduler.py:4343
           INFO     Scheduler closing all comms                scheduler.py:4371
           INFO     Retire worker addresses                    scheduler.py:7614
                    (stimulus_id='retire-workers-1768652962.98
                    42737') (0, 1, 2, 3)
           INFO     Closing Nanny at 'tcp://127.0.0.1:36567'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:37791'.       nanny.py:611
                    Reason: nanny-close
[12:29:23] INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:37773'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Closing Nanny at 'tcp://127.0.0.1:41685'.       nanny.py:611
                    Reason: nanny-close
           INFO     Nanny asking worker to close. Reason:           nanny.py:858
                    nanny-close
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:56578; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:56590; closing.
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:56602; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:36261  scheduler.py:5444
                    name: 0
                    (stimulus_id='handle-worker-cleanup-176865
                    2963.0512938')
           INFO     Remove worker addr: tcp://127.0.0.1:44047  scheduler.py:5444
                    name: 1
                    (stimulus_id='handle-worker-cleanup-176865
                    2963.060545')
           INFO     Remove worker addr: tcp://127.0.0.1:37313  scheduler.py:5444
                    name: 2
                    (stimulus_id='handle-worker-cleanup-176865
                    2963.0693061')
           INFO     Received 'close-stream' from                     core.py:908
                    tcp://127.0.0.1:56616; closing.
           INFO     Remove worker addr: tcp://127.0.0.1:44935  scheduler.py:5444
                    name: 3
                    (stimulus_id='handle-worker-cleanup-176865
                    2963.098891')
           INFO     Lost all workers                           scheduler.py:5582
           INFO     Nanny at 'tcp://127.0.0.1:36567' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:37791' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:41685' closed.        nanny.py:626
           INFO     Nanny at 'tcp://127.0.0.1:37773' closed.        nanny.py:626
           INFO     Closing scheduler. Reason: unknown         scheduler.py:4343
           INFO     Scheduler closing all comms                scheduler.py:4371
/app/.venv/lib/python3.12/site-packages/coverage/inorout.py:497: CoverageWarning:

Module dev_src was never imported. (module-not-imported); see https://coverage.readthedocs.io/en/7.13.1/messages.html#warning-module-not-imported

/app/.venv/lib/python3.12/site-packages/coverage/inorout.py:497: CoverageWarning:

Module src was never imported. (module-not-imported); see https://coverage.readthedocs.io/en/7.13.1/messages.html#warning-module-not-imported

/app/.venv/lib/python3.12/site-packages/coverage/control.py:958: CoverageWarning:

No data was collected. (no-data-collected); see https://coverage.readthedocs.io/en/7.13.1/messages.html#warning-no-data-collected

/app/.venv/lib/python3.12/site-packages/pytest_cov/plugin.py:363: CovReportWarning:

Failed to generate report: No data to report.
